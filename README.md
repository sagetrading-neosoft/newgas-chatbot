ğŸ§  PDF Chatbot with Elasticsearch & Ollama
This project is a real-time chatbot powered by Flask, Flask-SocketIO, Elasticsearch, and a local LLM (via Ollama). It ingests PDF documents, splits their content into searchable chunks, and allows users to query this content through a chat interface.

ğŸ“¦ Features
ğŸ“„ Extracts text from PDF documents

ğŸ” Indexes and searches using Elasticsearch

ğŸ¤– Uses a local LLM (e.g., LLaMA 3 via Ollama) for intelligent responses

ğŸ’¬ Real-time conversation using WebSockets (Flask-SocketIO)

ğŸ§  Maintains per-session conversation history

ğŸŒ CORS enabled for easy frontend integration

ğŸ› ï¸ Configurable via .env files for development and production

ğŸš€ Getting Started
1. Clone the Repository
bash
Copy
Edit
git clone https://github.com/yourusername/pdf-chatbot.git
cd pdf-chatbot
2. Set Up the Environment
Install dependencies:

bash
Copy
Edit
pip install -r requirements.txt
Make sure you have:

Python 3.8+

Elasticsearch running locally on localhost:9200

Ollama installed with your chosen model (e.g., llama3)

3. Add PDF Files
Place your PDFs in the pdf_data/ folder (or change the path in .env).

4. Environment Variables
Use the provided .env.dev or create your own .env.prod file.

Example: .env.dev

env
Copy
Edit
APP_ENV=prod
ES_HOST=http://localhost:9200
LLM_MODEL=llama3.2
CHUNK_SIZE=1024
TOP_K_RESULTS=5
CONVERSATION_LENGTH=5
PDF_FOLDER_PATH=pdf_data
INDEX_NAME=data_chunks
OLLAMA_VERSION=llama3.2
SECRET_KEY=dev-secret-key
5. Run the Application
bash
Copy
Edit
python app.py
The server will start on http://0.0.0.0:5000.

âš™ï¸ How It Works
ğŸ“„ PDF Processing
Reads all PDFs from PDF_FOLDER_PATH.

Extracts text using PyPDF2.

Splits text into ~CHUNK_SIZE character chunks using sentence boundaries.

Indexes chunks into Elasticsearch under the index INDEX_NAME.

ğŸ” Searching
When the user sends a query:

The app searches Elasticsearch for the top K relevant chunks.

Formats a prompt with:

Session history

System message guidelines

Matching chunks

User query

ğŸ§  LLM Response
The formatted prompt is sent to the Ollama LLM. The model responds with a message that is returned to the frontend via WebSocket.

ğŸ“¡ WebSocket Events
Event	Description
connect	Initializes a new session
disconnect	Cleans up session history
message	Receives a user message and emits response via response
Payload Example:

json
Copy
Edit
{
  "Messages": {
    "content": "What is our return policy?"
  }
}
ğŸ§  System Message Rules
The bot is designed to:

Act as a formal assistant named NewGas Assistant

Ignore context for greetings or simple affirmations

Avoid references to "context", "PDF", or "FAQ"

Never return an empty response

ğŸ” Security Notes
In production, store Elasticsearch credentials securely using .env.prod.

Disable verify_certs=False for secure Elasticsearch deployments.

ğŸ› ï¸ Tech Stack
Flask â€“ Lightweight backend framework

Flask-SocketIO â€“ Real-time communication

Elasticsearch â€“ Vector/text search

PyPDF2 â€“ PDF parsing

Ollama â€“ Local LLM execution

dotenv â€“ Configuration management

ğŸ“ Project Structure
bash
Copy
Edit
.
â”œâ”€â”€ app.py              # Main application
â”œâ”€â”€ pdf_data/           # Folder for input PDFs
â”œâ”€â”€ .env.dev            # Dev environment config
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ README.md           # This file
ğŸ“Œ TODO / Improvements
Add support for vector embeddings

Web-based chat frontend

Upload PDFs via API

Authentication & role-based access

ğŸ§ª Example Usage
makefile
Copy
Edit
User: What safety protocols are in place?
Assistant: Our safety protocols include mandatory training, regular audits, and equipment checks...
ğŸ“¬ Questions?
Feel free to open an issue or reach out!
